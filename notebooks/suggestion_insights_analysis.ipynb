{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c241c7c2-9358-4628-b998-2e6207c5f8f1",
   "metadata": {},
   "source": [
    "## Instala√ß√£o e Prepara√ß√£o do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec906ea7-5da3-49c1-a294-2da31475abb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install duckdb\n",
    "!pip install pandas\n",
    "!pip install dotenv\n",
    "!pip install -q -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4313030-65b2-4581-a575-711740b66bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timezone\n",
    "from zoneinfo import ZoneInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac6e26-88d8-40d8-99b1-4fed2bda053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "db_user = os.getenv(\"DB_USER\")\n",
    "db_password = os.getenv(\"DB_PASSWORD\")\n",
    "db_host = os.getenv(\"DB_HOST\")\n",
    "db_port = os.getenv(\"DB_PORT\")\n",
    "db_name = os.getenv(\"DB_NAME\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda31bbc-1b0a-4c2c-ad35-bc306d502b9b",
   "metadata": {},
   "source": [
    "## Extra√ß√£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba78a9c4-e2a7-4329-87b5-1239256e5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_STATUS = 'Aberto'\n",
    "TOP_N_SUBCATEGORIES = 3\n",
    "MAX_TICKETS_PER_COMPANY = 1000\n",
    "\n",
    "conn_string = f\"dbname={db_name} user={db_user} password={db_password} host={db_host} port={db_port}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83415df-8ecb-49cb-a10a-bfe8820ae1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    con = duckdb.connect(database=':memory:')\n",
    "    \n",
    "    con.execute(\"INSTALL postgres;\")\n",
    "    con.execute(\"LOAD postgres;\")\n",
    "    \n",
    "    con.execute(f\"ATTACH $${conn_string}$$ AS postgres_db (TYPE POSTGRES);\")\n",
    "    \n",
    "    print(\"DuckDB -> Postgres connection established successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deea0fc4-6180-402a-bae0-1c4b1bf0126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "WITH SubcategoryRanks AS (\n",
    "    SELECT\n",
    "        co.name AS company_name,\n",
    "        p.name AS product_name,\n",
    "        sc.name AS subcategory_name,\n",
    "        ROW_NUMBER() OVER(PARTITION BY co.name, p.name ORDER BY COUNT(t.ticket_id) DESC) as rank_num\n",
    "    FROM\n",
    "        postgres_db.public.tickets AS t\n",
    "    JOIN postgres_db.public.companies AS co ON t.company_id = co.company_id\n",
    "    JOIN postgres_db.public.products AS p ON t.product_id = p.product_id\n",
    "    JOIN postgres_db.public.subcategories AS sc ON t.subcategory_id = sc.subcategory_id\n",
    "    JOIN postgres_db.public.statuses AS st ON t.current_status_id = st.status_id\n",
    "    WHERE\n",
    "        st.name = '{OPEN_STATUS}'\n",
    "    GROUP BY\n",
    "        company_name, product_name, subcategory_name\n",
    "),\n",
    "\n",
    "CompanyQuota AS (\n",
    "    SELECT\n",
    "        company_name,\n",
    "        FLOOR({MAX_TICKETS_PER_COMPANY} / (COUNT(DISTINCT product_name) * {TOP_N_SUBCATEGORIES})) AS tickets_per_slot_quota\n",
    "    FROM\n",
    "        SubcategoryRanks\n",
    "    WHERE\n",
    "        rank_num <= {TOP_N_SUBCATEGORIES}\n",
    "    GROUP BY\n",
    "        company_name\n",
    "),\n",
    "\n",
    "RankedTickets AS (\n",
    "    SELECT\n",
    "        co.name AS company_name,\n",
    "        p.name AS product_name,\n",
    "        sc.name AS subcategory_name,\n",
    "        t.title,\n",
    "        t.description,\n",
    "        st.name AS status_name,\n",
    "        t.created_at,\n",
    "        cq.tickets_per_slot_quota,\n",
    "        ROW_NUMBER() OVER(PARTITION BY co.name, p.name, sc.name ORDER BY t.created_at DESC) as ticket_rank\n",
    "    FROM\n",
    "        postgres_db.public.tickets AS t\n",
    "    JOIN postgres_db.public.companies AS co ON t.company_id = co.company_id\n",
    "    JOIN postgres_db.public.products AS p ON t.product_id = p.product_id\n",
    "    JOIN postgres_db.public.subcategories AS sc ON t.subcategory_id = sc.subcategory_id\n",
    "    JOIN postgres_db.public.statuses AS st ON t.current_status_id = st.status_id\n",
    "    JOIN SubcategoryRanks sr ON co.name = sr.company_name AND p.name = sr.product_name AND sc.name = sr.subcategory_name\n",
    "    JOIN CompanyQuota cq ON co.name = cq.company_name\n",
    "    WHERE\n",
    "        sr.rank_num <= {TOP_N_SUBCATEGORIES}\n",
    "        AND st.name = '{OPEN_STATUS}'\n",
    "),\n",
    "\n",
    "UniqueTickets AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        ROW_NUMBER() OVER(PARTITION BY title, description ORDER BY created_at DESC) as unique_rank\n",
    "    FROM\n",
    "        RankedTickets\n",
    "    WHERE\n",
    "        ticket_rank <= tickets_per_slot_quota\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    company_name,\n",
    "    product_name,\n",
    "    subcategory_name,\n",
    "    title,\n",
    "    description,\n",
    "    status_name\n",
    "FROM\n",
    "    UniqueTickets\n",
    "WHERE\n",
    "    unique_rank = 1;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4449a4a0-c95e-4cea-8a30-54a3be87bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_chamados = con.execute(query).fetchdf()\n",
    "    print(f\"Total of {len(df_chamados)} tickets found\")\n",
    "\n",
    "    if df_chamados.empty:\n",
    "        print(\"No tickets found\")\n",
    "    else:\n",
    "        companhias = df_chamados['company_name'].unique()\n",
    "\n",
    "        output_dir = \"../data/original_tickets\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for comp in companhias:\n",
    "            df_companhia = df_chamados[df_chamados['company_name'] == comp]\n",
    "            \n",
    "            safe_filename = str(comp).lower().replace(' ', '_').replace('/', '_') + \".csv\"\n",
    "            output_path = os.path.join(output_dir, safe_filename)\n",
    "            \n",
    "            df_companhia.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a62649-3ba4-4bea-afef-74c9cfc7cf64",
   "metadata": {},
   "source": [
    "## Verifica√ß√£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7530948e-9db2-4de2-b439-744d0d5e03a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../data/original_tickets\"\n",
    "\n",
    "csv_pattern = os.path.join(input_dir, '*.csv')\n",
    "csv_files = glob.glob(csv_pattern)\n",
    "\n",
    "if not csv_files:\n",
    "    exit()\n",
    "\n",
    "all_dataframes = []\n",
    "\n",
    "for file_path in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        all_dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {file_path}: {e}\")\n",
    "\n",
    "if not all_dataframes:\n",
    "    exit()\n",
    "\n",
    "master_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "required_columns = ['company_name', 'product_name', 'subcategory_name']\n",
    "if not all(col in master_df.columns for col in required_columns):\n",
    "    exit()\n",
    "\n",
    "summary = master_df.groupby(['company_name', 'product_name', 'subcategory_name']).size().reset_index(name='total_chamados')\n",
    "\n",
    "\n",
    "print(\"\\n--- Ticket Report by Customer, Product, and Subcategory ---\")\n",
    "print(summary.to_string())\n",
    "\n",
    "output_report_path = '../data/report_tickets/grouped_report_v4.csv'\n",
    "try:\n",
    "    summary.to_csv(output_report_path, index=False, encoding='utf-8-sig')\n",
    "except Exception as e:\n",
    "    print(f\"\\nError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c3a5d-16e9-4e67-89dd-50822afbc48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../data/original_tickets\"\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    for filename in os.listdir(output_dir):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(output_dir, filename)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                all_dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {filename}: {e}\")\n",
    "\n",
    "    if all_dfs:\n",
    "        full_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "        duplicated_titles = full_df[full_df.duplicated(subset=['title'], keep=False)]\n",
    "        \n",
    "        if not duplicated_titles.empty:\n",
    "            print(f\"Found {duplicated_titles.shape[0]} records with duplicate titles.\")\n",
    "            print(duplicated_titles[['title', 'company_name']].head())\n",
    "        else:\n",
    "            print(\"No duplicate titles found.\")\n",
    "\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        duplicated_descriptions = full_df[full_df.duplicated(subset=['description'], keep=False)]\n",
    "\n",
    "        if not duplicated_descriptions.empty:\n",
    "            print(f\"Found {duplicated_descriptions.shape[0]} records with duplicate descriptions.\")\n",
    "            print(duplicated_descriptions[['description', 'company_name']].head())\n",
    "        else:\n",
    "            print(\"No duplicate descriptions found.\")\n",
    "    else:\n",
    "        print(\"No CSV files found for analysis.\")\n",
    "else:\n",
    "    print(f\"Directory '{output_dir}' not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0212b8a-b639-46b6-b53f-86f8e89f0dde",
   "metadata": {},
   "source": [
    "## Infer√™ncia LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2436387b-0eb8-4702-a238-3bb867d69fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "input_dir = \"../data/original_tickets\"\n",
    "\n",
    "TOP_N_THEMES = 3\n",
    "\n",
    "    \n",
    "def preparar_contexto_do_dataframe(df_produto):\n",
    "    \"\"\"Prepares the raw text from the tickets for the LLM.\"\"\"\n",
    "    contexto_textual = \"\"\n",
    "    for index, row in df_produto.iterrows():\n",
    "        contexto_textual += f\"--- Ticket {index + 1} (Subcategory: {row['subcategory_name']}) ---\\n\"\n",
    "        contexto_textual += f\"Title: {row['title']}\\n\"\n",
    "        contexto_textual += f\"Description: {row['description']}\\n\"\n",
    "    return contexto_textual\n",
    "\n",
    "def criar_prompt(contexto_chamados, company_name, product_name, subcategory_stats_str):\n",
    "    \"\"\"\n",
    "    Creates a prompt with statistics and requests an output in the format of insights and actions.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Voc√™ √© um Product Manager, com foco em an√°lise de dados para tomada de decis√£o.\n",
    "\n",
    "    **Contexto:**\n",
    "    Voc√™ est√° analisando os chamados de suporte para a empresa \"{company_name}\", focando no produto \"{product_name}\".\n",
    "    Uma an√°lise quantitativa pr√©via dos chamados revelou os seguintes temas (subcategorias) como os mais problem√°ticos:\n",
    "\n",
    "    **Estat√≠sticas dos Principais Problemas:**\n",
    "    {subcategory_stats_str}\n",
    "\n",
    "    **Sua Tarefa:**\n",
    "    Com base nas estat√≠sticas acima e nos detalhes dos chamados fornecidos abaixo, sua miss√£o √© gerar um plano de a√ß√£o conciso. Para cada um dos temas principais:\n",
    "    1.  Sintetize o problema central.\n",
    "    2.  Proponha uma lista de a√ß√µes claras, espec√≠ficas e implement√°veis para resolver a causa raiz desses problemas.\n",
    "\n",
    "    **Formato da Resposta:**\n",
    "    Sua resposta DEVE ser um objeto JSON v√°lido, sem nenhum texto ou formata√ß√£o adicional. A estrutura deve ser a seguinte:\n",
    "    {{\n",
    "      \"company_name\": \"{company_name}\",\n",
    "      \"product_name\": \"{product_name}\",\n",
    "      \"insights\": [\n",
    "        {{\n",
    "          \"theme\": \"Nome do Tema Principal 1 (Subcategoria)\",\n",
    "          \"percentage\": \"XX%\",\n",
    "          \"actions\": [\n",
    "            \"A√ß√£o sugerida 1 para o Tema 1.\",\n",
    "            \"A√ß√£o sugerida 2 para o Tema 1.\"\n",
    "          ]\n",
    "        }},\n",
    "        {{\n",
    "          \"theme\": \"Nome do Tema Principal 2 (Subcategoria)\",\n",
    "          \"percentage\": \"YY%\",\n",
    "          \"actions\": [\n",
    "            \"A√ß√£o sugerida 1 para o Tema 2.\"\n",
    "          ]\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "\n",
    "    **Dados Brutos dos Chamados para An√°lise Qualitativa:**\n",
    "    {contexto_chamados}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "\n",
    "csv_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]\n",
    "\n",
    "if not csv_files:\n",
    "    print(f\"No CSV files found in the directory '{input_dir}'.\")\n",
    "else:\n",
    "    for csv_file in csv_files:\n",
    "        caminho_completo = os.path.join(input_dir, csv_file)\n",
    "        print(f\"‚öôÔ∏è Processing client file: {csv_file}\")\n",
    "        \n",
    "        try:\n",
    "            df_cliente = pd.read_csv(caminho_completo)\n",
    "            \n",
    "            if df_cliente.empty:\n",
    "                print(\"   -> File is empty. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            company_name = df_cliente.iloc[0]['company_name']\n",
    "            \n",
    "            produtos_no_arquivo = df_cliente['product_name'].unique()\n",
    "            print(f\"   -> Found products: {list(produtos_no_arquivo)}\")\n",
    "\n",
    "            for product_name in produtos_no_arquivo:\n",
    "                sao_paulo_tz = ZoneInfo(\"America/Sao_Paulo\")\n",
    "                \n",
    "                start_time_utc = datetime.now(timezone.utc)\n",
    "                \n",
    "                print(f\"\\n   >> Analyzing product: '{product_name}' at {start_time_utc.isoformat()}...\")\n",
    "                \n",
    "                df_produto_especifico = df_cliente[df_cliente['product_name'] == product_name]\n",
    "                \n",
    "                print(f\"      -> Calculating statistics with Pandas...\")\n",
    "                subcategory_dist = df_produto_especifico['subcategory_name'].value_counts(normalize=True)\n",
    "                top_themes = subcategory_dist.head(TOP_N_THEMES)\n",
    "                stats_str = \"\"\n",
    "                for theme, percentage in top_themes.items():\n",
    "                    stats_str += f\"- **{theme}:** {percentage:.0%} of tickets\\n\"\n",
    "                print(f\"      -> Statistics found:\\n{stats_str}\")\n",
    "\n",
    "                contexto = preparar_contexto_do_dataframe(df_produto_especifico)\n",
    "                \n",
    "                if contexto:\n",
    "                    prompt_final = criar_prompt(contexto, company_name, product_name, stats_str)\n",
    "                    \n",
    "                    try:\n",
    "                        print(\"   üì≤ Sending enriched prompt to Gemini API...\")\n",
    "                        response = model.generate_content(prompt_final)\n",
    "                        \n",
    "                        cleaned_response = response.text.strip()\n",
    "                        json_start = cleaned_response.find('{')\n",
    "                        json_end = cleaned_response.rfind('}') + 1\n",
    "                        \n",
    "                        if json_start != -1 and json_end != -1:\n",
    "                            json_str = cleaned_response[json_start:json_end]\n",
    "                            try:\n",
    "                                parsed_json = json.loads(json_str)\n",
    "                                \n",
    "                                end_time_utc = datetime.now(timezone.utc)\n",
    "                                \n",
    "                                start_time_sp = start_time_utc.astimezone(sao_paulo_tz)\n",
    "                                end_time_sp = end_time_utc.astimezone(sao_paulo_tz)\n",
    "                                \n",
    "                                parsed_json['starttime'] = start_time_sp.isoformat()\n",
    "                                parsed_json['endtime'] = end_time_sp.isoformat()\n",
    "                                parsed_json['dth'] = end_time_sp.isoformat()\n",
    "                                \n",
    "                                print(\"\\n   ‚úÖ Final JSON object to be saved:\")\n",
    "                                print(json.dumps(parsed_json, indent=2, ensure_ascii=False))\n",
    "\n",
    "                            except json.JSONDecodeError:\n",
    "                                print(\"   ‚ùå The response is not valid JSON after cleaning:\", json_str)\n",
    "                        else:\n",
    "                            print(\"   ‚ùå No JSON found in the API response:\", cleaned_response)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"   ‚ùå Error calling Gemini API for product '{product_name}': {e}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Critical error processing file {csv_file}: {e}\")\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7886503-592e-49c8-ab18-a53f4c705b79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
